{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMhCOPgtSsSgw4HRE6DiNr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amukund8/AML/blob/main/CrossValidationPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I39qy308xiT9",
        "outputId": "ddf6e5b6-d939-4ef8-8f2a-a2ed46348c17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 99437018.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 58434970.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 30922832.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 15425529.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Epoch 1/5, Training Loss: 0.1556419895930605\n",
            "Epoch 2/5, Training Loss: 0.04574345427191457\n",
            "Epoch 3/5, Training Loss: 0.03186318325471438\n",
            "Epoch 4/5, Training Loss: 0.02185623975061595\n",
            "Epoch 5/5, Training Loss: 0.0176332447713641\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define a simple convolutional neural network model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "model = SimpleCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Set training parameters\n",
        "num_epochs = 5\n",
        "batch_size = 64\n",
        "\n",
        "# Download and preprocess the MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_loss}\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "from torch.utils.data.dataset import random_split\n",
        "from sklearn.model_selection import KFold\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "# Assuming you have a PyTorch dataset (MNIST in this example)\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "mnist_dataset = MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "\n",
        "# Set the number of folds\n",
        "num_folds = 5\n",
        "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_indices, val_indices) in enumerate(kf.split(mnist_dataset)):\n",
        "    print(f\"Fold {fold + 1}/{num_folds}\")\n",
        "\n",
        "    # Create data loaders for training and validation\n",
        "    train_dataset = Subset(mnist_dataset, train_indices)\n",
        "    val_dataset = Subset(mnist_dataset, val_indices)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "    # Define your model, optimizer, and loss function\n",
        "    # model = ...\n",
        "    # optimizer = ...\n",
        "    # criterion = ...\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 5\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for inputs, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            total_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            for inputs, targets in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += targets.size(0)\n",
        "                correct += (predicted == targets).sum().item()\n",
        "\n",
        "            average_loss = total_loss / len(val_loader)\n",
        "            accuracy = correct / total\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {average_loss}, Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "# After training on all folds, you can evaluate the model's overall performance or perform other tasks.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dMTkmlrx4mc",
        "outputId": "75ccaa7e-f1ac-4f0b-e54b-581e47313f11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5\n",
            "Epoch 1/5, Validation Loss: 0.0161185667878494, Accuracy: 99.46%\n",
            "Epoch 2/5, Validation Loss: 0.020146102601098934, Accuracy: 99.31%\n",
            "Epoch 3/5, Validation Loss: 0.016303761175491826, Accuracy: 99.52%\n",
            "Epoch 4/5, Validation Loss: 0.020477583818812994, Accuracy: 99.44%\n",
            "Epoch 5/5, Validation Loss: 0.01716038114072267, Accuracy: 99.49%\n",
            "\n",
            "\n",
            "Fold 2/5\n",
            "Epoch 1/5, Validation Loss: 0.008750520539764538, Accuracy: 99.62%\n",
            "Epoch 2/5, Validation Loss: 0.009595420062476266, Accuracy: 99.67%\n",
            "Epoch 3/5, Validation Loss: 0.011053679421568999, Accuracy: 99.66%\n",
            "Epoch 4/5, Validation Loss: 0.009130321026822593, Accuracy: 99.66%\n",
            "Epoch 5/5, Validation Loss: 0.006407821414332496, Accuracy: 99.79%\n",
            "\n",
            "\n",
            "Fold 3/5\n",
            "Epoch 1/5, Validation Loss: 0.00369500464106075, Accuracy: 99.88%\n",
            "Epoch 2/5, Validation Loss: 0.0028442484647060984, Accuracy: 99.90%\n",
            "Epoch 3/5, Validation Loss: 0.003654712354916604, Accuracy: 99.88%\n",
            "Epoch 4/5, Validation Loss: 0.016427748200237957, Accuracy: 99.64%\n",
            "Epoch 5/5, Validation Loss: 0.006928442251699699, Accuracy: 99.83%\n",
            "\n",
            "\n",
            "Fold 4/5\n",
            "Epoch 1/5, Validation Loss: 0.005653603082009795, Accuracy: 99.79%\n",
            "Epoch 2/5, Validation Loss: 0.0015909821008503974, Accuracy: 99.96%\n",
            "Epoch 3/5, Validation Loss: 0.004184041216729881, Accuracy: 99.83%\n",
            "Epoch 4/5, Validation Loss: 0.005656994248354637, Accuracy: 99.77%\n",
            "Epoch 5/5, Validation Loss: 0.006507301365058377, Accuracy: 99.79%\n",
            "\n",
            "\n",
            "Fold 5/5\n",
            "Epoch 1/5, Validation Loss: 0.0017052887011181662, Accuracy: 99.96%\n",
            "Epoch 2/5, Validation Loss: 0.0012104943753252656, Accuracy: 99.94%\n",
            "Epoch 3/5, Validation Loss: 0.0026664149025271955, Accuracy: 99.90%\n",
            "Epoch 4/5, Validation Loss: 0.0025235777914071663, Accuracy: 99.92%\n",
            "Epoch 5/5, Validation Loss: 0.007246682280045307, Accuracy: 99.80%\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for fold, (train_indices, val_indices) in enumerate(kf.split(mnist_dataset)):\n",
        "    print(f\"Fold {fold + 1}/{num_folds}\")\n",
        "\n",
        "    # Create data loaders for training and validation\n",
        "    train_dataset = Subset(mnist_dataset, train_indices)\n",
        "    print(len(train_dataset))\n",
        "    val_dataset = Subset(mnist_dataset, val_indices)\n",
        "    print(len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzl-93WWDcwg",
        "outputId": "09425e93-7400-4d3d-b3a8-fcf439674a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5\n",
            "48000\n",
            "12000\n",
            "Fold 2/5\n",
            "48000\n",
            "12000\n",
            "Fold 3/5\n",
            "48000\n",
            "12000\n",
            "Fold 4/5\n",
            "48000\n",
            "12000\n",
            "Fold 5/5\n",
            "48000\n",
            "12000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5UrLIvqfRVzz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}